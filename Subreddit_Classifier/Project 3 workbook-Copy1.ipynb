{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3 - Reddit Challenge\n",
    "\n",
    "# Contents:\n",
    "- [1.0 Data Science Problem:](#Data-Science-Problem:)\n",
    "- [1.1 Executive Summary:](#Executive-Summary:)\n",
    "- [1.2 Importing libaries](#Importing-libraries) \n",
    "- [1.3 Scraping Reddit using pushshift api (https://github.com/pushshift/api)](#Scraping-Reddit-using-pushshift-api-(https://github.com/pushshift/api))\n",
    "- [2. Data cleaning: Initial check](#Data-cleaning:-Initial-check)\n",
    "- [2.1 Outliers](#Outliers)\n",
    "- [2.2 Create data cleaning function](#Create-data-cleaning-function)\n",
    "- [3. EDA: View Plots](#EDA:-View-Plots)\n",
    "- [3.1 EDA: Plot a Heatmap of the Correlation Matrix](#EDA:-Plot-a-Heatmap-of-the-Correlation-Matrix)\n",
    "- [3.2 EDA: Use seaborn's .pairplot() method to create scatterplots for each of our features on the target 'SalePrice'](#EDA:-Use-seaborn's-.pairplot()-method-to-create-scatterplots-for-each-of-our-features-on-the-target-'SalePrice')\n",
    "- [4. Modeling: Starting Train/Test split](#Modeling:-Starting-Train/Test-split)\n",
    "- [4.1 Ridge model train/test](#Ridge-model-train/test)\n",
    "- [4.2 Ridge with log](#Ridge-with-log)\n",
    "- [4.3 Lasso model train/test](#Lasso-model-train/test)\n",
    "- [4.4 Lasso with log](#Lasso-with-log)\n",
    "- [4.5 Lasso Log Model with all data](#Lasso-Log-Model-with-all-data)\n",
    "- [4.6 View Lasso Log Residuals from all data](#View-Lasso-Log-Residuals-from-all-data)\n",
    "- [4.7 Additional Plots](#Additional-Plots)\n",
    "- [4.8 Lasso model statistics](#Lasso-model-statistics)\n",
    "- [5. Load Kaggle Test Data](#Load-Kaggle-Test-Data)\n",
    "- [5.1 Clean test data (match training data)](#Clean-test-data-(match-training-data))\n",
    "- [5.2 Apply lasso model and generate predictions](#Apply-lasso-model-and-generate-predictions)\n",
    "- [5.3 Save SalesPrice Kaggle submission to csv](#Save-SalesPrice-Kaggle-submission-to-csv)\n",
    "- [5.4 Conclusion:](#Conclusion:)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Problem:\n",
    "How can we use 2006-2010 data from residential properties sold in Ames, Iowa to predict future home sales using a regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary:\n",
    "\n",
    "The Ames Housing Dataset is an exceptionally detailed and robust dataset with over 70 columns of different features relating to houses.\n",
    "Data set contains information from the Ames Assessorâ€™s Office used in computing assessed values for individual residential properties sold in Ames, IA from 2006 to 2010.\n",
    "\n",
    "The data has 82 columns which include 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Reddit using pushshift api (https://github.com/pushshift/api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = []\n",
    "# subreddit = []\n",
    "# upvotes = []\n",
    "# comments = []\n",
    "# subscribers = []\n",
    "# created_date = []\n",
    "# url = []\n",
    "# id = []\n",
    "\n",
    "# df = pd.DataFrame(columns = ['title', 'subreddit', 'comments', 'upvotes', 'subscribers', 'created_date', 'url', 'id'])\n",
    "\n",
    "# after = None\n",
    "\n",
    "# for a in range(20):\n",
    "#     print(f\"Looping through page {a+1}\")\n",
    "#     if after == None:\n",
    "#         params = {\n",
    "#         'subreddit': 'thanosdidnothingwrong',\n",
    "#         'after': '1531195200',\n",
    "#         'size': 1000,\n",
    "#         'score': '>0',\n",
    "#         'num_comments': '>0'\n",
    "#         }\n",
    "#     else:\n",
    "#         headers = {'User-agent': 'Alex'}\n",
    "#         params = {\n",
    "#         'subreddit': 'thanosdidnothingwrong',\n",
    "#         'after': after,\n",
    "#         'size': 1000,\n",
    "#         'score': '>0',\n",
    "#         'num_comments': '>0'\n",
    "#         }\n",
    "#     link = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "#     res = requests.get(link, params=params, headers=headers)\n",
    "#     if res.status_code == 200:\n",
    "#         the_json = res.json()\n",
    "#         for i in range(len(the_json['data'])):\n",
    "#                     title.append(the_json['data'][i]['title'])\n",
    "#                     subreddit.append(the_json['data'][i]['subreddit'])\n",
    "#                     comments.append(the_json['data'][i]['num_comments'])\n",
    "#                     upvotes.append(the_json['data'][i]['score'])\n",
    "#                     subscribers.append(the_json['data'][i]['subreddit_subscribers'])\n",
    "#                     created_date.append(the_json['data'][i]['created_utc'])\n",
    "#                     url.append(the_json['data'][i]['full_link'])\n",
    "#                     id.append(the_json['data'][i]['id'])\n",
    "#     after = the_json['data'][i]['created_utc']\n",
    "#     time.sleep(1)\n",
    "\n",
    "# df['title'] = title\n",
    "# df['subreddit'] = subreddit\n",
    "# df['comments'] = comments\n",
    "# df['upvotes'] = upvotes\n",
    "# df['subscribers'] = subscribers\n",
    "# df['created_date'] = created_date\n",
    "# df['url'] = url\n",
    "# df['id'] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = []\n",
    "# subreddit = []\n",
    "# upvotes = []\n",
    "# comments = []\n",
    "# subscribers = []\n",
    "# created_date = []\n",
    "# url = []\n",
    "# id = []\n",
    "\n",
    "# df1 = pd.DataFrame(columns = ['title', 'subreddit', 'comments', 'upvotes', 'subscribers', 'created_date', 'url', 'id'])\n",
    "\n",
    "# after = None\n",
    "\n",
    "# for a in range(17):\n",
    "#     print(f\"Looping through page {a+1}\")\n",
    "#     if after == None:\n",
    "#         params = {\n",
    "#         'subreddit': 'inthesoulstone',\n",
    "#         'after': '1531195200',\n",
    "#         'size': 1000,\n",
    "#         'score': '>0',\n",
    "#         'num_comments': '>0'\n",
    "#         }\n",
    "#     else:\n",
    "#         headers = {'User-agent': 'Alex'}\n",
    "#         params = {\n",
    "#         'subreddit': 'inthesoulstone',\n",
    "#         'after': after,\n",
    "#         'size': 1000,\n",
    "#         'score': '>0',\n",
    "#         'num_comments': '>0'\n",
    "#         }\n",
    "#     link = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "#     res = requests.get(link, params=params, headers=headers)\n",
    "#     if res.status_code == 200:\n",
    "#         the_json = res.json()\n",
    "#         for i in range(len(the_json['data'])):\n",
    "#                     title.append(the_json['data'][i]['title'])\n",
    "#                     subreddit.append(the_json['data'][i]['subreddit'])\n",
    "#                     comments.append(the_json['data'][i]['num_comments'])\n",
    "#                     upvotes.append(the_json['data'][i]['score'])\n",
    "#                     subscribers.append(the_json['data'][i]['subreddit_subscribers'])\n",
    "#                     created_date.append(the_json['data'][i]['created_utc'])\n",
    "#                     url.append(the_json['data'][i]['full_link'])\n",
    "#                     id.append(the_json['data'][i]['id'])\n",
    "#     after = the_json['data'][i]['created_utc']\n",
    "#     time.sleep(1)\n",
    "\n",
    "# df1['title'] = title\n",
    "# df1['subreddit'] = subreddit\n",
    "# df1['comments'] = comments\n",
    "# df1['upvotes'] = upvotes\n",
    "# df1['subscribers'] = subscribers\n",
    "# df1['created_date'] = created_date\n",
    "# df1['url'] = url\n",
    "# df1['id'] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6a861a8b3f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine `df` and `df1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([df, df1],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.loc[19999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[combined_data['upvotes'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[9167]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_data.to_csv('./data/combined_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: \n",
    "Load csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/combined_data.csv')\n",
    "data.drop('Unnamed: 0',axis=1, inplace=True) #drop first column Unnamed: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>created_date</th>\n",
       "      <th>url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For those who fell... for balance</td>\n",
       "      <td>thanosdidnothingwrong</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>638343</td>\n",
       "      <td>1531195214</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xla5n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join here if banned or spared.</td>\n",
       "      <td>thanosdidnothingwrong</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>638334</td>\n",
       "      <td>1531195219</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xla6c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow.</td>\n",
       "      <td>thanosdidnothingwrong</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>638275</td>\n",
       "      <td>1531195240</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xlaah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I lived?</td>\n",
       "      <td>thanosdidnothingwrong</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>638248</td>\n",
       "      <td>1531195248</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xlac2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test</td>\n",
       "      <td>thanosdidnothingwrong</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>638207</td>\n",
       "      <td>1531195265</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xlafc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title              subreddit  comments  \\\n",
       "0  For those who fell... for balance  thanosdidnothingwrong         1   \n",
       "1     Join here if banned or spared.  thanosdidnothingwrong         3   \n",
       "2                               Wow.  thanosdidnothingwrong         4   \n",
       "3                           I lived?  thanosdidnothingwrong         6   \n",
       "4                               Test  thanosdidnothingwrong         1   \n",
       "\n",
       "   upvotes  subscribers  created_date  \\\n",
       "0        2       638343    1531195214   \n",
       "1        1       638334    1531195219   \n",
       "2        1       638275    1531195240   \n",
       "3        1       638248    1531195248   \n",
       "4        1       638207    1531195265   \n",
       "\n",
       "                                                 url      id  \n",
       "0  https://www.reddit.com/r/thanosdidnothingwrong...  8xla5n  \n",
       "1  https://www.reddit.com/r/thanosdidnothingwrong...  8xla6c  \n",
       "2  https://www.reddit.com/r/thanosdidnothingwrong...  8xlaah  \n",
       "3  https://www.reddit.com/r/thanosdidnothingwrong...  8xlac2  \n",
       "4  https://www.reddit.com/r/thanosdidnothingwrong...  8xlafc  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thanosdidnothingwrong    0.540789\n",
       "inthesoulstone           0.459211\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:  \n",
    "Use get dummies on `subreddit` column\n",
    "- 1 = thanos did nothing wrong\n",
    "- 0 = in the soulstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.subreddit = pd.get_dummies(data.subreddit,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>created_date</th>\n",
       "      <th>url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For those who fell... for balance</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>638343</td>\n",
       "      <td>1531195214</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xla5n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join here if banned or spared.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>638334</td>\n",
       "      <td>1531195219</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xla6c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>638275</td>\n",
       "      <td>1531195240</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xlaah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I lived?</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>638248</td>\n",
       "      <td>1531195248</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xlac2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>638207</td>\n",
       "      <td>1531195265</td>\n",
       "      <td>https://www.reddit.com/r/thanosdidnothingwrong...</td>\n",
       "      <td>8xlafc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  subreddit  comments  upvotes  \\\n",
       "0  For those who fell... for balance          1         1        2   \n",
       "1     Join here if banned or spared.          1         3        1   \n",
       "2                               Wow.          1         4        1   \n",
       "3                           I lived?          1         6        1   \n",
       "4                               Test          1         1        1   \n",
       "\n",
       "   subscribers  created_date  \\\n",
       "0       638343    1531195214   \n",
       "1       638334    1531195219   \n",
       "2       638275    1531195240   \n",
       "3       638248    1531195248   \n",
       "4       638207    1531195265   \n",
       "\n",
       "                                                 url      id  \n",
       "0  https://www.reddit.com/r/thanosdidnothingwrong...  8xla5n  \n",
       "1  https://www.reddit.com/r/thanosdidnothingwrong...  8xla6c  \n",
       "2  https://www.reddit.com/r/thanosdidnothingwrong...  8xlaah  \n",
       "3  https://www.reddit.com/r/thanosdidnothingwrong...  8xlac2  \n",
       "4  https://www.reddit.com/r/thanosdidnothingwrong...  8xlafc  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Feature and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['title'] #feature\n",
    "y = data['subreddit'] #target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state =42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nice_conmat(y_test, preds):\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy: {accuracy_score(y_test, preds)}')\n",
    "    return pd.DataFrame(conf_matrix, columns=['Predicted ' + str(i) for i in ng.target_names],\\\n",
    "            index=['Actual ' + str(i) for i in ng.target_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "- with `CountVectorizer`\n",
    "- with `TFIDF Vectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 33.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6661554604891436\n",
      "0.6603850880786563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 75,\n",
       " 'rf__n_estimators': 200,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__stop_words': None}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('rf', RandomForestClassifier())\n",
    "# ])\n",
    "\n",
    "# params = {\n",
    "#     'vect__ngram_range': [(1, 2)],\n",
    "#     'vect__stop_words': [None, 'english'],\n",
    "#     'vect__min_df': [1,2,4],\n",
    "#     'rf__n_estimators':[50,100,200],\n",
    "#     'rf__max_depth':[25,50,75]\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(pipe, params, verbose=2, cv=5, n_jobs=-1)\n",
    "\n",
    "# gs.fit(X_train, y_train)\n",
    "\n",
    "# y_preds = gs.predict(X_test)\n",
    "\n",
    "# print(gs.best_score_)\n",
    "# print(accuracy_score(y_test, y_preds))\n",
    "# gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   55.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-bf9d6354cdd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfidf__ngram_range': [(1, 3)],\n",
    "    'tfidf__stop_words': [None, 'english'],\n",
    "    'tfidf__min_df': [1,2,4],\n",
    "    'rf__n_estimators':[50,100,200],\n",
    "    'rf__max_depth':[25,50,75]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, verbose=2, cv=5, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "y_preds = gs.predict(X_test)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/alexnguyen/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  38.1s\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  37.6s\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  40.7s\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  41.2s\n",
      "[CV] svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  41.5s\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.4min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.3min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.3min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.3min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.4min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.7min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.7min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.7min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.7min\n",
      "[CV] svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=1, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.8min\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.2min\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.2min\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.2min\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  40.5s\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  41.7s\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  36.9s\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  38.4s\n",
      "[CV] svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  46.0s\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.5min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.4min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.3min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.4min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.4min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.7min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.8min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.9min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.9min\n",
      "[CV] svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=2, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=english, total= 1.9min\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.2min\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.3min\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=None, total= 1.2min\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  36.2s\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  36.8s\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  43.3s\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  39.4s\n",
      "[CV] svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english \n",
      "[CV]  svm__C=3, svm__kernel=rbf, vect__ngram_range=(1, 3), vect__stop_words=english, total=  36.6s\n",
      "[CV] svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.6min\n",
      "[CV] svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.5min\n",
      "[CV] svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.5min\n",
      "[CV] svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n",
      "[CV]  svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None, total= 4.3min\n",
      "[CV] svm__C=3, svm__kernel=linear, vect__ngram_range=(1, 3), vect__stop_words=None \n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range': [(1, 3)],\n",
    "    'vect__stop_words': [None, 'english'],\n",
    "    'svm__C': [1, 2, 3],\n",
    "    'svm__kernel': ['rbf', 'linear']\n",
    "}\n",
    "gs = GridSearchCV(pipe, params, verbose=2, cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "y_preds = gs.predict(X_test)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfidf__ngram_range': [(1, 3)],\n",
    "    'tfidf__stop_words': [None, 'english'],\n",
    "    'svm__C': [1, 2, 3, 4],\n",
    "    'svm__kernel': ['rbf', 'poly', 'linear']\n",
    "}\n",
    "gs = GridSearchCV(pipe, params, verbose=2, cv=5,n_jobs=2)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "y_preds = gs.predict(X_test)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2a629bb3e37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlog_tfidf_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_tfidf_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_tfidf_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mlog_tfidf_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_tfidf_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_tfidf_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "log_tfidf_params = {\n",
    "    'tfidf__ngram_range': [(1,3)],\n",
    "    'tfidf__stop_words': [None, 'english'],\n",
    "    'log_reg__penalty': ['l1', 'l2'],\n",
    "    'log_reg__C': [1,10,100]\n",
    "}\n",
    "\n",
    "log_tfidf_gs = GridSearchCV(log_tfidf_pipe, log_tfidf_params, verbose=2, cv=5, n_jobs=-1)\n",
    "\n",
    "log_tfidf_gs.fit(X_train, y_train)\n",
    "\n",
    "y_preds = log_tfidf_gs.predict(X_test)\n",
    "\n",
    "print(log_tfidf_gs.best_score_)\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "log_tfidf_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:  3.0min finished\n",
      "/Users/alexnguyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657760916942449\n",
      "0.6578451454322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'log_reg__C': 1,\n",
       " 'log_reg__penalty': 'l2',\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__stop_words': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_vect_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "log_vect_params = {\n",
    "    'vect__ngram_range': [(1, 3)],\n",
    "    'vect__stop_words': [None, 'english'],\n",
    "    'log_reg__penalty': ['l1', 'l2'],\n",
    "    'log_reg__C': [1,10,100]\n",
    "}\n",
    "\n",
    "log_vect_gs = GridSearchCV(log_vect_pipe, log_vect_params, verbose=2, cv=5, n_jobs=2)\n",
    "\n",
    "log_vect_gs.fit(X_train, y_train)\n",
    "\n",
    "y_preds = log_vect_gs.predict(X_test)\n",
    "\n",
    "print(log_vect_gs.best_score_)\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "log_vect_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6548147550246186\n",
      "0.6551413355182303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ada__base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " 'ada__n_estimators': 100,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__stop_words': None}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('ada', AdaBoostClassifier())\n",
    "# ])\n",
    "\n",
    "# params = {\n",
    "#     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#     'vect__stop_words': [None, 'english'],\n",
    "#     'ada__base_estimator': [DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=2),DecisionTreeClassifier(max_depth=3)],\n",
    "#     'ada__n_estimators': [50, 100]\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(pipe, params, verbose=2, cv=5, n_jobs=-1)\n",
    "\n",
    "# gs.fit(X_train, y_train)\n",
    "\n",
    "# y_preds = gs.predict(X_test)\n",
    "\n",
    "# print(gs.best_score_)\n",
    "# print(accuracy_score(y_test, y_preds))\n",
    "# gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6500524658971668\n",
      "0.6498156493240476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gb__loss': 'exponential',\n",
       " 'gb__max_depth': 4,\n",
       " 'gb__n_estimators': 100,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__stop_words': None}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__ngram_range': [(1, 2)],\n",
    "    'vect__stop_words': [None, 'english'],\n",
    "    'gb__loss': ['deviance', 'exponential'],\n",
    "    'gb__n_estimators': [50, 100],\n",
    "    'gb__max_depth': [3, 4]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, verbose=2, cv=5, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "y_preds = gs.predict(X_test)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 22.2min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 31.8min finished\n",
      "/Users/alexnguyen/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6634110904834934\n",
      "0.6651372388365424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'log_reg__C': 1,\n",
       " 'log_reg__penalty': 'l2',\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 3),\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': 'unicode'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag_pipe = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('bag', BaggingClassifier())\n",
    "# ])\n",
    "\n",
    "# bag_params = {\n",
    "#     'vect__ngram_range': [(1, 2)],\n",
    "#     'vect__stop_words': [None, 'english'],\n",
    "#     'vect__strip_accents': ['ascii', 'unicode'],\n",
    "#     'bag__base_estimator': [DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3), DecisionTreeClassifier(max_depth=4)],\n",
    "#     'bag__n_estimators': [10, 50, 100],\n",
    "# }\n",
    "\n",
    "# bag_gs = GridSearchCV(pipe, params, verbose=2, cv=5, n_jobs=-1)\n",
    "\n",
    "# bag_gs.fit(X_train, y_train)\n",
    "\n",
    "# y_preds = bag_gs.predict(X_test)\n",
    "\n",
    "# print(bag_gs.best_score_)\n",
    "# print(accuracy_score(y_test, y_preds))\n",
    "# bag_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('bag', BaggingClassifier())\n",
    "])\n",
    "\n",
    "bag_params = {\n",
    "    'tfidf__ngram_range': [(1, 2)],\n",
    "    'tfidf__stop_words': [None, 'english'],\n",
    "    'tfidf__strip_accents': ['ascii', 'unicode'],\n",
    "    'bag__base_estimator': [DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3), DecisionTreeClassifier(max_depth=4)],\n",
    "    'bag__n_estimators': [10, 50, 100],\n",
    "}\n",
    "\n",
    "bag_gs = GridSearchCV(pipe, params, verbose=2, cv=5, n_jobs=-1)\n",
    "\n",
    "bag_gs.fit(X_train, y_train)\n",
    "\n",
    "y_preds = bag_gs.predict(X_test)\n",
    "\n",
    "print(bag_gs.best_score_)\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "bag_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f8290b70d653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoef_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcoef_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Absolute'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcoef_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bag' is not defined"
     ]
    }
   ],
   "source": [
    "coef_df = pd.DataFrame(bag.coef_, columns=cv1.get_feature_names()).T\n",
    "coef_df['Absolute'] = coef_df[0].abs()\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "bayes.pipe = Pipeline([\n",
    "    ('normalizer', TextNormalizer()),\n",
    "    ('vectorizer', GensimVectorizer()),\n",
    "    ('bayes', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(onehot__threshold=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search = GridSearchCV(model, param_grid={\n",
    "    'count__analyzer': ['word', 'char', 'char_wb'],\n",
    "    'count__ngram_range': [(1,1), (1,2), (1,3), (1,4), (1,5), (2,3)],\n",
    "    'onehot__threshold': [0.0, 1.0, 2.0, 3.0],\n",
    "    'bayes__alpha': [0.0, 1.0],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "# this normalizes each term frequency by the \n",
    "# number of documents having that term\n",
    "tfidf = TfidfVectorizer()\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect',vect),\n",
    "    ('tfidf',tfidf),\n",
    "    ('log_reg')\n",
    "])\n",
    "\n",
    "# call fit as you would on any classifier\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "# predict test instances\n",
    "y_preds = pipeline.predict(X_test)\n",
    "\n",
    "# calculate f1\n",
    "mean_f1 = f1_score(y_test, y_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'vect__max_df':[0.8,0.9,1.0],\n",
    "    'clf__C':[0.1,1.0]\n",
    "}\n",
    "\n",
    "# do 3-fold cross validation for each of the 6 possible\n",
    "# combinations of the parameter values above\n",
    "grid = GridSearchCV(pipeline, cv=3, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
